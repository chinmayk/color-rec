\Subsection{image-color-source}{Images as a Source of Color Data}
\system's pipeline starts by obtaining a set of topic-related images. \system assumes that images related to a topic will contain the topic's characteristic colors and so, sampling pixels from these images is correlates with sampling color values from the topic's color space. \system uses Google Images as its image source because it indexes a large number of images  (unlike, say Flickr, which consists primarily of user uploaded photographs), and because it does not require images to be tagged explicitly (unlike ImageNet), which increases the diversity of the corpus. However, indexed images vary in quality, size and topic relevance and the number of images per search is limited by the API (to 32).\system is largely robust to these these problems, as described below. 

\Subsection{sampling-images}{Sampling Images}
Given a set of images related to a topic, \system then randomly samples pixels from these images. \system uses simple population sampling $(S_{P})$ and uniformly samples a fixed number of pixels from each image . Unlike other sampling schemes, this requires no knowledge of the ``natural'' distribution of colors in images, nor are pre-processing steps such as edge-detection required. Population sampling may lead to over-weighting of color values that occur frequently in general. We handle this through query expansion.

\Subsection{query-expansion}{Query Expansion}
In addition to images related to the given topic $t$ ($I(t)$), \system also queries Google Images for a set of similar topics (say $T'$), and samples them to obtain ($S_{P}(I(T'))$). Since the topics are similar, we assume their color distributions are similar. By ``subtracting'' the general distribution from the sampled distribution for the topic, \system obtains a color distribution that is more specific to $t$. \system makes the simplifying assumption that observed frequencies of color for a topic ($Obs(t)$) are a linear combination of the true topic-specific distribution $C(t)$, and the distribution for similar topics $T'$. Since $C(t)$ is non-negative, we clamp this value at zero (we use $\alpha = 0.15$).
\begin{align}
Obs(t) &\approx \alpha\cdot C(T') + (1-\alpha)\cdot C(t) \\
\label{linear-color}
\implies  C(t) &\approx max\left\{0,\frac{Obs(t) - \alpha\cdot C(T')}{(1-\alpha)}\right\}
\end{align}

Even while colors are perceptually similar, their color values may differ. Instead of subtracting raw frequencies, we bin color-values in LAB space, and subtract bin-frequencies (Euclidean distance between two color coordinates in LAB color space approximates the perceptual difference). We use a bin size twice the just-noticeable difference in each dimension. After binning, we set the color value of the bin to the the color-value in the sample that is closest to its centroid to ensure we don't introduce additional colors. Equation \ref{linear-color} modifies to ($B$ is the binning operator):
\begin{align}
\label{linear-color-bin}  
B(C(t)) &\approx max\left\{0,\frac{B(Obs(t)) - \alpha \cdot B(C(T'))}{(1-\alpha)}\right\}
\end{align}

\Subsection{clustering}{Clustering Color Values}
While $B(C(t))$ approximates the color-distribution of the topic, we still need to obtain individual colors that best represent it. The color distribution for a topic can be considered as a mixture model, where the final distribution comes from one of several (say $n$) independent component distributions chosen from with known probabilities. In such a model, the representative colors will be the means of the components. 

We tried three approaches to obtain components. First, we fit a general gaussian mixture model [cite] to $B(C(t))$. The perceptually valid region of the LAB color space is small and GMMs often fail for $n>3$ gaussians. Second, we used gaussian mixtures with shared covariances (so components have the same shape, but may differ in size) which work with larger values of $n$. Third, we used K-Means clustering.

We observed that K-Means and shared-covariance Gaussians performed equally well, but K-Means was much faster. Therefore, \system uses K-Means clustering. Clustering is done in LAB space and results in a number of clusters in the color space whose centroids are the means of the component Gaussians. Bins are assigned to the cluster whose centroid they are closest to. 

\system considers the centroid of each cluster as a candidate color for the topic, so the quality of the clustering affects results significantly (Based on its initial seeds, K-Means can converge to different clusters). \system does not use labeled topic-relevance data, and so cannot evaluate if clustering was done ``correctly''. Instead, we try to maximize attributes associated with correct clusters. First, clusters should be \textit{dense}, so each represents a perceptually coherent color. Second, cluster-centroids must be \textit{widely separated}, so the colors obtained aren't just variations of each other. 

The Davies-Boudin index tries to balance these two criteria~\cite{davies1979cluster}.  A lower value of DB indicates better clustering. Below, $\sigma_{i}, \sigma_{j}$ are the average distances of points in cluster $i$ and $j$ from the respective centroids, $d(c_{i}, c_{j})$ is the distance between cluster centroids.
\begin{align}
DB &= \frac{1}{n} \sum_{i=1, i \neq j}^{n}max\left(\frac{\sigma_{i} + \sigma_{j}}{d(c_{i}, c_{j})}\right)
\end{align}
The potential for cluster separation is limited since the LAB color space is finite. Therefore, the cluster density is more important than cluster separation, and we use a modified version of the Davies-Boudin index that uses the square of distances instead of distances themselves. \system runs K-Means clustering 32 times and picks as candidate colors the centroids that minimize $DB'$. 

\Subsection{choosing-palettes}{Choosing Palettes}
Once candidate colors are identified, \system also attempts to find palettes if multiple topics are provided as inputs. To form a palette, \system picks one color from the candidate colors for each topic using a Gauss-Seidel method for optimization~\cite{dwyer2009scalable}. The objective function can either be frequency of occurrence of the chosen colors, their saturation or their perceptual separation. In each case, \system constrains the chosen colors such that they are separated perceptually by a given distance (we use 25 times the just-noticeable difference). In cases where such constraints are impossible to satisfy, \system performs a number of iterations of Gauss-Seidel to obtain a ``good-enough'' palette.