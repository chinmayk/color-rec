TODO Julie. NOTE: Use APA format.
Relevance, Likability (and Concrete/abstract)

We tested two hypotheses. First, we wanted to see if our algorithm gets us the right set of colors. To test this hypothesis, we showed 50 participants from Mechanical Turk six options for each topic. Four of them were colors generated by Colorific. One was a random color picked from the Protovis palette, an industry standard in data visualization. The last choice was none of the above. Participants picked one choice that best suited the topic. We found that the Colorific colors were preferred in an overwhelming majority (X2 = 83.7562, df = 2, p < 0.001). 947 times out of 1200, Turkers picked a Colorific color. So, Colorific is able to pick at least some good colors for a topic. 

Now, we wanted to test if we can combine colors from different football teams, like Cardinal, Cal Bears, and others to create a single, cohesive palette. We hypothesized that the algorithm can choose palettes that are liked better than random palettes. We werenÕt sure of the best way to pick one color, so we tried four variations. One tried to pick the most frequent color, another optimized for saturation, one maximized perceptual distance between the colors, and the last picked colors from the topic at random. We now asked participants to rate our four variations, and one random palette from Protovis on a scale of 1-7.






We evaluated the system on three related metrics: the likability of the generated color palettes, how topic-relevant the palettes were perceived to be, and how the colors in the palette affect understanding of the data they represent. For all three metrics, the algorithmically generated palettes were compared against a randomly generated palette, and one generated by experts. For the likability and understanding metrics, the random palette was chosen from the set of palettes generated for other topics by our system. This was to ensure that only the relevance, not the base quality of the colors was considered. For all topics tested, we limited the number of specific items represented in the palette to four. This also allowed us to compare the algorithmically and randomly generated palettes to the randomly generated palettes. We ran a small laboratory study of X participants recruited through school mailing lists, in addition to a large-scale crowdsourced study on Amazon's Mechanical Turk. 

\subsection{Likability}
To measure likability, the automatically, expert, and randomly generated color palettes for a given topic are presented in a random order. Participants rate each palette on a seven-point Likert scale based on how much they like each palette for a given topic. 

\subsection{Relevance}
For relevance, an association task is used: given a topic (e.g "US Politics") and one of the topic terms (e.g. "Democrat"), the participant chooses which color, among a set of displayed swatches, is relevant to it.

\subsection{Understanding}
For understanding, users will be shown differently-colored infographics, and participants will be timed while they answer conceptual questions related to the infographic. Since the three metrics may interact strongly, they will be studied in a within-subjects design.